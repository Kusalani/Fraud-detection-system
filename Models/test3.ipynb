{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data.drop(columns=['isFlaggedFraud'], inplace=True)\n",
    "    data[\"type\"] = data[\"type\"].map({\"CASH_OUT\": 1, \"PAYMENT\": 2, \"CASH_IN\": 3, \"TRANSFER\": 4, \"DEBIT\": 5})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def engineer_features(df):\n",
    "    df['amount_to_oldbalanceOrg'] = df['amount'] / (df['oldbalanceOrg'] + 1)\n",
    "    df['amount_to_newbalanceOrig'] = df['amount'] / (df['newbalanceOrig'] + 1)\n",
    "    df['balance_change_ratio_orig'] = (df['newbalanceOrig'] - df['oldbalanceOrg']) / (df['oldbalanceOrg'] + 1)\n",
    "    df['balance_change_ratio_dest'] = (df['newbalanceDest'] - df['oldbalanceDest']) / (df['oldbalanceDest'] + 1)\n",
    "    df['transaction_freq_orig'] = df.groupby('nameOrig')['step'].transform('count')\n",
    "    df['transaction_freq_dest'] = df.groupby('nameDest')['step'].transform('count')\n",
    "    df['avg_amount_orig'] = df.groupby('nameOrig')['amount'].transform('mean')\n",
    "    df['avg_amount_dest'] = df.groupby('nameDest')['amount'].transform('mean')\n",
    "    df['amount_to_avg_orig'] = df['amount'] / df['avg_amount_orig']\n",
    "    df['amount_to_avg_dest'] = df['amount'] / df['avg_amount_dest']\n",
    "    df['hour'] = df['step'] % 24\n",
    "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "    return df.drop(['nameOrig', 'nameDest'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VAE model\n",
    "def build_vae(input_dim, encoding_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(32, activation='relu')(inputs)\n",
    "    z_mean = layers.Dense(encoding_dim)(x)\n",
    "    z_log_var = layers.Dense(encoding_dim)(x)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], encoding_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "    \n",
    "    decoder = layers.Dense(32, activation='relu')(z)\n",
    "    outputs = layers.Dense(input_dim)(decoder)\n",
    "\n",
    "    vae = models.Model(inputs, outputs)\n",
    "\n",
    "    reconstruction_loss = tf.reduce_mean(tf.square(inputs - outputs))\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = load_and_preprocess_data(\"onlinefraud .csv\")\n",
    "data_engineered = engineer_features(data)\n",
    "data_cleaned = data_engineered.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "X = data_cleaned.drop('isFraud', axis=1)\n",
    "y = data_cleaned['isFraud']\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resampling strategy\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "oversampler = SMOTENC(categorical_features=[1], sampling_strategy=0.7, random_state=42)  # Assuming 'type' is the second column\n",
    "\n",
    "# Create resampling pipeline\n",
    "resampling_pipeline = Pipeline([\n",
    "    ('undersampler', undersampler),\n",
    "    ('oversampler', oversampler)\n",
    "])\n",
    "\n",
    "# Apply resampling to training data only\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original training dataset shape:\", X_train_scaled.shape)\n",
    "print(\"Resampled training dataset shape:\", X_train_resampled.shape)\n",
    "print(\"\\nResampled training set distribution:\\n\", pd.Series(y_train_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 10\n",
    "vae = build_vae(input_dim, encoding_dim)\n",
    "vae.fit(X_train_scaled, epochs=50, batch_size=128, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Compute reconstruction error\n",
    "reconstructions = vae.predict(X_train_scaled)\n",
    "mse = np.mean(np.square(X_train_scaled - reconstructions), axis=1)\n",
    "threshold = np.percentile(mse, 95)  # Adjust this percentile as needed\n",
    "\n",
    "# Add reconstruction error as a feature\n",
    "X_train_with_re = np.column_stack((X_train_scaled, mse))\n",
    "X_test_with_re = np.column_stack((X_test_scaled, np.mean(np.square(X_test_scaled - vae.predict(X_test_scaled)), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define resampling strategy\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "oversampler = SMOTENC(categorical_features=[1], sampling_strategy=0.7, random_state=42)  # Assuming 'type' is the second column\n",
    "\n",
    "# Create resampling pipeline\n",
    "resampling_pipeline = Pipeline([\n",
    "    ('undersampler', undersampler),\n",
    "    ('oversampler', oversampler)\n",
    "])\n",
    "\n",
    "# Apply resampling\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_with_re, y_train)\n",
    "\n",
    "print(\"Original dataset shape:\", X_train_with_re.shape)\n",
    "print(\"Resampled dataset shape:\", X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "svm_classifier = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n",
    "svm_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test_with_re)\n",
    "y_pred_proba = svm_classifier.predict_proba(X_test_with_re)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC AUC Score:\")\n",
    "print(roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': list(X.columns) + ['reconstruction_error'],\n",
    "    'coefficient': np.abs(svm_classifier.coef_[0])\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='coefficient', y='feature', data=feature_importance.head(15))\n",
    "plt.title('Top 15 Feature Coefficients (Absolute Value)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
